# Feature-Fusion Neck Model for Content-Based Histopathological Image Retrieval

## Abstract
The extraction of feature descriptors in histopathological images poses a significant challenge for the implementation of Content-Based Image Retrieval (CBIR) systems, crucial tools for assisting pathologists. This complexity arises from the diverse types of tissues and the high dimensionality of Whole Slide Images (WSIs). However, the advent of deep learning models, such as Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), has greatly improved the extraction of these feature descriptors. These models typically generate embeddings by leveraging deeper single-scale linear layers or advanced pooling layers, significantly enhancing the accuracy and efficiency of feature extraction. This improvement enables more precise and scalable CBIR systems that better support pathologists in their diagnostic processes. Despite these advancements, the embeddings, which focus on local spatial details at a single scale, miss out on the richer spatial context from earlier layers. This gap highlights the need for methods that incorporate multi-scale information to enhance the depth and utility of feature descriptors in histopathological image analysis. In this work, we propose the Local-Global Feature Fusion Embedding Model (LGFFEM), an approach composed of a pre-trained backbone for feature extraction from multi-scales, a neck branch for local-global feature fusion, and a Generalized Mean (GeM)-based pooling head for feature descriptors. Our experiments involved training the model's neck and head on ImageNet-1k and PanNuke datasets using Sub-center ArcFace loss. We demonstrate the model's performance by benchmarking it against state-of-the-art results using the Kimia Path24C dataset for histopathological image retrieval, where it achieves a Recall@1 rate of 99.40% for test patches.